{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_T=pd.read_csv('C://Users//antoi//Dropbox//PC//Desktop//Projet 5//data_T.csv')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "list_of_all_tags = []\n",
    "for item in data_T['tags']:\n",
    "    list_of_all_tags.append(item)\n",
    "\n",
    "tags=np.unique(list_of_all_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cluster, metrics\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m manifold, decomposition\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Projet 5\\venv\\lib\\site-packages\\sklearn\\cluster\\__init__.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mThe :mod:`sklearn.cluster` module gathers popular unsupervised clustering\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03malgorithms.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_spectral\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m spectral_clustering, SpectralClustering\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_mean_shift\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mean_shift, MeanShift, estimate_bandwidth, get_bin_seeds\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_affinity_propagation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m affinity_propagation, AffinityPropagation\n",
      "File \u001B[1;32m~\\PycharmProjects\\Projet 5\\venv\\lib\\site-packages\\sklearn\\cluster\\_spectral.py:15\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_random_state, as_float_array\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecated\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpairwise\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pairwise_kernels\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneighbors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m kneighbors_graph, NearestNeighbors\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmanifold\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m spectral_embedding\n",
      "File \u001B[1;32m~\\PycharmProjects\\Projet 5\\venv\\lib\\site-packages\\sklearn\\metrics\\__init__.py:7\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mand pairwise metrics and distance computations.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ranking\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m auc\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ranking\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m average_precision_score\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ranking\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m coverage_error\n",
      "File \u001B[1;32m~\\PycharmProjects\\Projet 5\\venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:37\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparsefuncs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m count_nonzero\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m UndefinedMetricWarning\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m label_binarize\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_encode\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _encode, _unique\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     41\u001B[0m     _average_binary_score,\n\u001B[0;32m     42\u001B[0m     _average_multiclass_ovo_score,\n\u001B[0;32m     43\u001B[0m     _check_pos_label_consistency,\n\u001B[0;32m     44\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Projet 5\\venv\\lib\\site-packages\\sklearn\\preprocessing\\__init__.py:37\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_label\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiLabelBinarizer\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_discretization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KBinsDiscretizer\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_polynomial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PolynomialFeatures\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_polynomial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SplineTransformer\n\u001B[0;32m     41\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBinarizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunctionTransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpower_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     70\u001B[0m ]\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:991\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:971\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:914\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(name, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1342\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1314\u001B[0m, in \u001B[0;36m_get_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1439\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(self, fullname, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:87\u001B[0m, in \u001B[0;36m_path_stat\u001B[1;34m(path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn import manifold, decomposition\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lecture dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l_cat = tags.tolist()\n",
    "print(\"catégories : \", l_cat)\n",
    "y_cat_num = [(l_cat.index(data_T['tags'][i])) for i in range(len(data_T))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Traitement\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag of word - Tf-idf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Préparation sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# création du bag of words (CountVectorizer et Tf-idf)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "ctf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "feat = 'keywords_bow_lem'\n",
    "cv_fit = cvect.fit(data_T[feat])\n",
    "ctf_fit = ctf.fit(data_T[feat])\n",
    "\n",
    "cv_transform = cvect.transform(data_T[feat])\n",
    "ctf_transform = ctf.transform(data_T[feat])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_bowcv=pd.DataFrame(cv_transform.toarray(),columns=cvect.get_feature_names())\n",
    "df_bowcv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_bowctf=pd.DataFrame(ctf_transform.toarray(),columns=ctf.get_feature_names())\n",
    "df_bowctf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fonctions pour réaliser l'ACP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_scree_plot(pca):\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ACP pour le bag of words CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# choix du nombre de composantes à calculer\n",
    "\n",
    "\n",
    "# préparation des données pour l'ACP\n",
    "X = df_bowcv.values\n",
    "features = df_bowcv.columns\n",
    "\n",
    "# Centrage et Réduction\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaledcv = std_scale.transform(X)\n",
    "\n",
    "# Calcul des composantes principales\n",
    "pcacv = decomposition.PCA()\n",
    "pcacv.fit_transform(X_scaledcv)\n",
    "\n",
    "# Eboulis des valeurs propres\n",
    "display_scree_plot(pcacv)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ACP pour le bag of words Tf-idf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# choix du nombre de composantes à calculer\n",
    "\n",
    "\n",
    "# préparation des données pour l'ACP\n",
    "X = df_bowctf.values\n",
    "features = df_bowctf.columns\n",
    "\n",
    "# Centrage et Réduction\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaledctf = std_scale.transform(X)\n",
    "\n",
    "# Calcul des composantes principales\n",
    "pcactf = decomposition.PCA()\n",
    "pcactf.fit_transform(X_scaledctf)\n",
    "\n",
    "# Eboulis des valeurs propres\n",
    "display_scree_plot(pcactf)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du modèle Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_size=500\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 100 # adapt to length of sentences\n",
    "sentences = data_T['keywords_bow_lem'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)\n",
    "#                                                workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Préparation des sentences (tokenization)\n",
    "\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post')\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création de la matrice d'embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 500\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "\n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation du modèle Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')\n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)\n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings = embed_model.predict(x_sentences)\n",
    "embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "import os\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.test.is_built_with_cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fonctions communes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True,\n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "\n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0],\n",
    "                             bert_inp['token_type_ids'][0],\n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "\n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "\n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size],\n",
    "                                                                      bert_tokenizer, max_length)\n",
    "\n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids,\n",
    "                                 \"input_mask\" : attention_mask,\n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "\n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "\n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2,'s')\n",
    "\n",
    "    return features_bert, last_hidden_states_tot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT HuggingFace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 'bert-base-uncased'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "sentences = data_T['keywords_dl'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création des features\n",
    "\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences,\n",
    "                                                         max_length, batch_size, mode='HF')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_bert.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## USE - Universal Sentence Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2,'s')\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = data_T['keywords_dl'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_USE = feature_USE_fct(sentences, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fonctions de tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn import manifold, decomposition\n",
    "import logging\n",
    "import time\n",
    "import sklearn.naive_bayes\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Calcul Tsne, détermination des clusters et calcul Jaccard entre vrais catégorie et n° de clusters\n",
    "\n",
    "\n",
    "def Jaccard_fct_bayes(features) :\n",
    "    time1 = time.time()\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    # Détermination des clusters à partir des données après Tsne\n",
    "    cls = sklearn.naive_bayes.BernoulliNB()\n",
    "    cls.fit(x_train,y_train)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,cls.predict(x_test),average='weighted')\n",
    "    print(cls.predict(x_test))\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"Jaccard : \", Jaccard, \"time : \", time2)\n",
    "\n",
    "    return Jaccard, X_tsne, cls.classes_, cls\n",
    "\n",
    "def Jaccard_fct_LogReg(features) :\n",
    "    time1 = time.time()\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    # Détermination des clusters à partir des données après Tsne\n",
    "    cls = sklearn.linear_model.LogisticRegression()\n",
    "    cls.fit(x_train,y_train)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,cls.predict(x_test),average='weighted')\n",
    "    print(cls.predict(x_test))\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"Jaccard : \", Jaccard, \"time : \", time2)\n",
    "\n",
    "    return Jaccard, X_tsne, cls.classes_, cls\n",
    "\n",
    "\n",
    "def Jaccard_fct_OvR(features) :\n",
    "    time1 = time.time()\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    # Détermination des clusters à partir des données après Tsne\n",
    "    cls = sklearn.multiclass.OneVsRestClassifier(SVC())\n",
    "    cls.fit(x_train,y_train)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,cls.predict(x_test),average='weighted')\n",
    "    print(cls.predict(x_test))\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"Jaccard : \", Jaccard, \"time : \", time2)\n",
    "\n",
    "    return Jaccard, X_tsne, cls.classes_, cls\n",
    "\n",
    "def Jaccard_fct_Forest(features) :\n",
    "    time1 = time.time()\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    # Détermination des clusters à partir des données après Tsne\n",
    "    cls = sklearn.ensemble.RandomForestClassifier()\n",
    "    cls.fit(x_train,y_train)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,cls.predict(x_test),average='weighted')\n",
    "    print(cls.predict(x_test))\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"Jaccard : \", Jaccard, \"time : \", time2)\n",
    "\n",
    "    return Jaccard, X_tsne, cls.classes_, cls\n",
    "\n",
    "\n",
    "# visualisation du Tsne selon les vraies catégories et selon les clusters\n",
    "def TSNE_visu_fct(X_tsne, y_cat_num, labels, Jaccard) :\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=y_cat_num, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=l_cat, loc=\"best\", title=\"Categorie\")\n",
    "    plt.title('Représentation des tags par catégories réelles')\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=labels, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=set(labels), loc=\"best\", title=\"Clusters\")\n",
    "    plt.title('Représentation des tags par clusters')\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Jaccard : \", Jaccard)\n",
    "\n",
    "\n",
    "def ScoreFunction(ListFeatures=[X_scaledcv,X_scaledctf,embeddings,features_bert,features_USE],ListFunctions=[Jaccard_fct_bayes,Jaccard_fct_LogReg,Jaccard_fct_OvR,Jaccard_fct_Forest]):\n",
    "    score=pd.DataFrame(columns=['CountVectorized','Tf-idf','Word2Vec','BERT','USE'],index=['NaiveBayes','LogisticRegression','KNeighbours','RandomForest'])\n",
    "    models=pd.DataFrame(columns=['CountVectorized','Tf-idf','Word2Vec','BERT','USE'],index=['NaiveBayes','LogisticRegression','KNeighbours','RandomForest'])\n",
    "    for i in range(len(ListFeatures)):\n",
    "        for k in range(len(ListFunctions)):\n",
    "            score.iloc[k,i],tsne,labels,models.iloc[k,i]=ListFunctions[k](ListFeatures[i])\n",
    "    return score, models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Score,Models=ScoreFunction()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Préparation des modèles pré-établis à partir des features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Jaccard, X_tsne, labels, clsBayes = Jaccard_fct_bayes(features_USE)\n",
    "Jaccard, X_tsne, labels, clsReg = Jaccard_fct_LogReg(features_USE)\n",
    "Jaccard, X_tsne, labels, clsOvR = Jaccard_fct_OvR(features_USE)\n",
    "Jaccard, X_tsne, labels, clsForest = Jaccard_fct_Forest(features_USE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Traitement\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag of word - Tf-idf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Préparation sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_T.dropna(axis=0,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_T.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# création du bag of words (CountVectorizer et Tf-idf)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "ctf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "feat = 'sentence_bow_lem'\n",
    "cv_fit = cvect.fit(data_T[feat])\n",
    "ctf_fit = ctf.fit(data_T[feat])\n",
    "\n",
    "cv_transform = cvect.transform(data_T[feat])\n",
    "ctf_transform = ctf.transform(data_T[feat])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_bowcv=pd.DataFrame(cv_transform.toarray(),columns=cvect.get_feature_names())\n",
    "df_bowcv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_bowctf=pd.DataFrame(ctf_transform.toarray(),columns=ctf.get_feature_names())\n",
    "df_bowctf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ACP pour le bag of words CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# choix du nombre de composantes à calculer\n",
    "\n",
    "\n",
    "# préparation des données pour l'ACP\n",
    "X = df_bowcv.values\n",
    "features = df_bowcv.columns\n",
    "\n",
    "# Centrage et Réduction\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaledcv_prep = std_scale.transform(X)\n",
    "\n",
    "# Calcul des composantes principales\n",
    "pcacv = decomposition.PCA()\n",
    "pcacv.fit_transform(X_scaledcv)\n",
    "\n",
    "# Eboulis des valeurs propres\n",
    "display_scree_plot(pcacv)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ACP pour le bag of words Tf-idf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# choix du nombre de composantes à calculer\n",
    "\n",
    "\n",
    "# préparation des données pour l'ACP\n",
    "X = df_bowctf.values\n",
    "features = df_bowctf.columns\n",
    "\n",
    "# Centrage et Réduction\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaledctf_prep = std_scale.transform(X)\n",
    "\n",
    "# Calcul des composantes principales\n",
    "pcactf = decomposition.PCA()\n",
    "pcactf.fit_transform(X_scaledctf)\n",
    "\n",
    "# Eboulis des valeurs propres\n",
    "display_scree_plot(pcactf)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du modèle Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_size=500\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 100 # adapt to length of sentences\n",
    "sentences = data_T['sentence_bow_lem'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)\n",
    "#                                                workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Préparation des sentences (tokenization)\n",
    "\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post')\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création de la matrice d'embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 500\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "\n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation du modèle Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')\n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)\n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings_prep = embed_model.predict(x_sentences)\n",
    "embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "import os\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"-1\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tf.test.is_built_with_cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fonctions communes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True,\n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "\n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0],\n",
    "                             bert_inp['token_type_ids'][0],\n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "\n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "\n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size],\n",
    "                                                                      bert_tokenizer, max_length)\n",
    "\n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids,\n",
    "                                 \"input_mask\" : attention_mask,\n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "\n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "\n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2,'s')\n",
    "\n",
    "    return features_bert, last_hidden_states_tot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT HuggingFace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 'bert-base-uncased'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "sentences = data_T['sentence_dl'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création des features\n",
    "\n",
    "features_bert_prep, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences,\n",
    "                                                         max_length, batch_size, mode='HF')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_bert.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## USE - Universal Sentence Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2,'s')\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = data_T['sentence_dl'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_USE_prep = feature_USE_fct(sentences, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applications aux features généralisées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def JaccardPrepBayes(features):\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,clsBayes.predict(x_test),average='weighted')\n",
    "    return Jaccard\n",
    "\n",
    "def JaccardPrepLogReg(features):\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,clsReg.predict(x_test),average='weighted')\n",
    "    return Jaccard\n",
    "\n",
    "def JaccardPrepOvR(features):\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,clsOvR.predict(x_test),average='weighted')\n",
    "    return Jaccard\n",
    "\n",
    "def JaccardPrepForest(features):\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    minlen=min(X_tsne.shape[0],data_T['tags'].shape[0])\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X_tsne[:minlen],data_T['tags'].values[:minlen],test_size=0.25)\n",
    "    Jaccard = sklearn.metrics.jaccard_score(y_test,clsForest.predict(x_test),average='weighted')\n",
    "    return Jaccard\n",
    "\n",
    "def ScoreFunctionPrep(ListFeatures=[X_scaledcv_prep,X_scaledctf_prep,embeddings_prep,features_bert_prep,features_USE_prep],ListFunctions=[JaccardPrepBayes,JaccardPrepLogReg,JaccardPrepOvR,JaccardPrepForest]):\n",
    "    score=pd.DataFrame(columns=['CountVectorized','Tf-idf','Word2Vec','BERT','USE'],index=['NaiveBayes','LogisticRegression','KNeighbours','RandomForest'])\n",
    "    for i in range(len(ListFeatures)):\n",
    "        for k in range(len(ListFunctions)):\n",
    "            score.iloc[k,i]=(ListFunctions[k])(ListFeatures[i])\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ScoreUpdated=ScoreFunctionPrep()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Scores,Models2=ScoreFunction(ListFeatures=[X_scaledcv_prep,X_scaledctf_prep,embeddings_prep,features_bert_prep,features_USE_prep])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ScoreUpdated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_USE=np.save('features_USE.csv',features_USE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4b71e1ff",
   "language": "python",
   "display_name": "PyCharm (Projet 5)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}